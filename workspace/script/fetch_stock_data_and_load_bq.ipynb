{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8b0cd4e-02f8-4e54-9a2c-6d63b7f93d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最新データが既に存在します。新しいデータはありません。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed       | 0/4062 [00:00<?, ?it/s]\n",
      "\n",
      "1 Failed download:\n",
      "['6543.T']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2024-10-13 -> 2024-10-13)')\n",
      "  0%|                                                                      | 0/4062 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for 6543.T. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed/4062 [00:01<1:09:37,  1.03s/it]\n",
      "\n",
      "1 Failed download:\n",
      "['3478.T']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "  0%|                                                            | 1/4062 [00:01<1:09:37,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for 3478.T. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed/4062 [00:02<1:09:54,  1.03s/it]\n",
      "\n",
      "1 Failed download:\n",
      "['1485.T']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2024-10-13 -> 2024-10-13)')\n",
      "  0%|                                                            | 2/4062 [00:02<1:09:54,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for 1485.T. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed/4062 [00:03<1:09:49,  1.03s/it]\n",
      "\n",
      "1 Failed download:\n",
      "['3975.T']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
      "  0%|                                                            | 3/4062 [00:03<1:09:49,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for 3975.T. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed/4062 [00:04<1:09:48,  1.03s/it]\n",
      "\n",
      "1 Failed download:\n",
      "['1486.T']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2024-10-13 -> 2024-10-13)')\n",
      "  0%|                                                            | 4/4062 [00:04<1:09:48,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for 1486.T. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 4/4062 [00:05<1:29:57,  1.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 151\u001b[0m\n\u001b[1;32m    147\u001b[0m         tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_dfが空です。処理をスキップします。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 127\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([combined_df, df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# Yahoo Finance APIの制限を避けるために1秒スリープ\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m combined_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# すべての銘柄データを一度にまとめてParquetファイルとして保存\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import datetime as dt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GCSBigQueryFacade:\n",
    "    def __init__(self, project_id, dataset_name, table_name, bucket_name):\n",
    "        self.project_id = project_id\n",
    "        self.dataset_name = dataset_name\n",
    "        self.table_name = table_name\n",
    "        self.bucket_name = bucket_name\n",
    "        self.bq_client = bigquery.Client()\n",
    "        self.storage_client = storage.Client()\n",
    "\n",
    "    def get_max_date_from_bq(self):\n",
    "        query = f\"\"\"\n",
    "            SELECT MAX(Date) as max_date \n",
    "            FROM `{self.project_id}.{self.dataset_name}.{self.table_name}`\n",
    "        \"\"\"\n",
    "        query_job = self.bq_client.query(query)\n",
    "        results = query_job.result()\n",
    "        for row in results:\n",
    "            return row['max_date']\n",
    "\n",
    "    def upload_to_gcs(self, local_file_path, destination_blob_name):\n",
    "        bucket = self.storage_client.bucket(self.bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(local_file_path)\n",
    "        tqdm.write(f\"File {local_file_path} uploaded to {destination_blob_name}.\")\n",
    "\n",
    "    def delete_from_gcs(self, file_name):\n",
    "        bucket = self.storage_client.bucket(self.bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "        if blob.exists():\n",
    "            blob.delete()\n",
    "            tqdm.write(f\"Deleted {file_name} from GCS.\")\n",
    "\n",
    "def suppress_yfinance_warnings():\n",
    "    import logging\n",
    "    yf_logger = logging.getLogger(\"yfinance\")\n",
    "    yf_logger.setLevel(logging.ERROR)\n",
    "\n",
    "def main():\n",
    "    suppress_yfinance_warnings()\n",
    "    \n",
    "    # 環境変数で認証情報を設定\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./dbt-analytics-engineer-435907-75a25995915e.json\"\n",
    "    \n",
    "    # BigQueryとGCSの設定\n",
    "    PROJECT_ID = 'dbt-analytics-engineer-435907'\n",
    "    DATASET_NAME = 'stock_dataset'\n",
    "    BUCKET_NAME = 'stock-data-bucket_hopop'\n",
    "    TABLE_NAME = 'stock_data'\n",
    "\n",
    "    # GCS & BigQuery操作のためのファサードを初期化\n",
    "    gcs_bq = GCSBigQueryFacade(PROJECT_ID, DATASET_NAME, TABLE_NAME, BUCKET_NAME)\n",
    "\n",
    "    # BigQueryから最大の日付を取得\n",
    "    max_date = gcs_bq.get_max_date_from_bq()\n",
    "    \n",
    "    if max_date:\n",
    "        # max_dateを文字列からdatetime型に変換\n",
    "        max_date = pd.to_datetime(max_date).date()\n",
    "        START_DATE = max_date + dt.timedelta(days=2)\n",
    "    else:\n",
    "        # データがない場合、デフォルトの開始日\n",
    "        START_DATE = dt.date(2024, 1, 1)\n",
    "\n",
    "    END_DATE = dt.date.today()\n",
    "\n",
    "    # START_DATE == END_DATEの場合、処理をスキップ\n",
    "    if START_DATE >= END_DATE:\n",
    "        tqdm.write(\"最新データが既に存在します。新しいデータはありません。\")\n",
    "        return\n",
    "\n",
    "    # GCS上のファイルが存在するかを確認し、削除\n",
    "    combined_file_name = \"combined_stock_data.parquet\"\n",
    "    gcs_bq.delete_from_gcs(combined_file_name)\n",
    "\n",
    "    # CSVファイルのパス\n",
    "    STOCK_MAPPING_CSV = '../stock_code_name_mapping.csv'\n",
    "\n",
    "    # CSVファイルの読み込み\n",
    "    stock_names_df = pd.read_csv(STOCK_MAPPING_CSV, usecols=['code', 'name'])\n",
    "\n",
    "    # すべての銘柄のデータを結合するためのデータフレームを準備\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # tqdmを使用して進捗を可視化（バーを最上部に固定）\n",
    "    progress_bar = tqdm(stock_names_df.iterrows(), total=len(stock_names_df), ncols=100, leave=True, position=0)\n",
    "\n",
    "    for index, row in progress_bar:\n",
    "        stock_code = str(row['code']).strip()\n",
    "        ticker = f\"{stock_code}.T\"  # 東証の場合、ティッカーは通常「.T」が付加されます\n",
    "        \n",
    "        # 株価データの取得\n",
    "        df = yf.download(ticker, start=START_DATE, end=END_DATE)\n",
    "\n",
    "        if df.empty:\n",
    "            tqdm.write(f\"No data found for {ticker}. Skipping...\")\n",
    "        else:\n",
    "            # データフレームの前処理\n",
    "            df.reset_index(inplace=True)\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Date',\n",
    "                'Open': 'Open',\n",
    "                'High': 'High',\n",
    "                'Low': 'Low',\n",
    "                'Close': 'Close',\n",
    "                'Adj Close': 'Adj_Close',\n",
    "                'Volume': 'Volume'\n",
    "            })\n",
    "            df['Stock_Code'] = stock_code\n",
    "    \n",
    "            # 必要なカラムのみを選択\n",
    "            df = df[['Date', 'Stock_Code', 'Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']]\n",
    "            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # データを結合\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        \n",
    "        # Yahoo Finance APIの制限を避けるために1秒スリープ\n",
    "        time.sleep(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "\n",
    "    if not combined_df.empty:\n",
    "        # すべての銘柄データを一度にまとめてParquetファイルとして保存\n",
    "        local_file_name = f\"combined_stock_data.parquet\"\n",
    "        local_file_path = f\"./output/{local_file_name}\"\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        \n",
    "        # Parquet形式で保存\n",
    "        combined_df.to_parquet(local_file_path, engine='pyarrow', index=False)\n",
    "        \n",
    "        # GCSへのアップロード\n",
    "        gcs_bq.upload_to_gcs(local_file_path, combined_file_name)\n",
    "        \n",
    "        # BigQueryへのデータロード\n",
    "        source_uri = f\"gs://{BUCKET_NAME}/{combined_file_name}\"\n",
    "        gcs_bq.load_data_to_bigquery(source_uri)\n",
    "    else:\n",
    "        tqdm.write(\"combined_dfが空です。処理をスキップします。\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f20e61-d670-4bb8-819f-dd155c91f394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
