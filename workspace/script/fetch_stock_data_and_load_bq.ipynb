{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b0cd4e-02f8-4e54-9a2c-6d63b7f93d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import datetime as dt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "class GCSBigQueryFacade:\n",
    "    def __init__(self, project_id, dataset_name, table_name, bucket_name):\n",
    "        self.project_id = project_id\n",
    "        self.dataset_name = dataset_name\n",
    "        self.table_name = table_name\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "        # 環境変数からサービスアカウント情報を取得\n",
    "        service_account_info = {\n",
    "            \"type\": \"service_account\",\n",
    "            \"project_id\": os.environ.get(\"GCP_PROJECT_ID\"),\n",
    "            \"private_key_id\": os.environ.get(\"GCP_PRIVATE_KEY_ID\"),\n",
    "            \"private_key\": os.environ.get(\"GCP_PRIVATE_KEY\").replace('\\\\n', '\\n'),\n",
    "            \"client_email\": os.environ.get(\"GCP_CLIENT_EMAIL\"),\n",
    "            \"client_id\": os.environ.get(\"GCP_CLIENT_ID\"),\n",
    "            \"auth_uri\": os.environ.get(\"GCP_AUTH_URI\"),\n",
    "            \"token_uri\": os.environ.get(\"GCP_TOKEN_URI\"),\n",
    "            \"auth_provider_x509_cert_url\": os.environ.get(\"GCP_AUTH_PROVIDER_X509_CERT_URL\"),\n",
    "            \"client_x509_cert_url\": os.environ.get(\"GCP_CLIENT_X509_CERT_URL\")\n",
    "        }\n",
    "        credentials = service_account.Credentials.from_service_account_info(service_account_info)\n",
    "\n",
    "        # BigQueryクライアントとStorageクライアントの初期化\n",
    "        self.bq_client = bigquery.Client(credentials=credentials, project=service_account_info[\"project_id\"])\n",
    "        self.storage_client = storage.Client(credentials=credentials, project=service_account_info[\"project_id\"])\n",
    "\n",
    "    def get_max_date_from_bq(self):\n",
    "        query = f\"\"\"\n",
    "            SELECT MAX(Date) as max_date \n",
    "            FROM `{self.project_id}.{self.dataset_name}.{self.table_name}`\n",
    "        \"\"\"\n",
    "        query_job = self.bq_client.query(query)\n",
    "        results = query_job.result()\n",
    "        for row in results:\n",
    "            return row['max_date']\n",
    "\n",
    "    def upload_to_gcs(self, local_file_path, destination_blob_name):\n",
    "        bucket = self.storage_client.bucket(self.bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(local_file_path)\n",
    "        tqdm.write(f\"File {local_file_path} uploaded to {destination_blob_name}.\")\n",
    "\n",
    "    def delete_from_gcs(self, file_name):\n",
    "        bucket = self.storage_client.bucket(self.bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "        if blob.exists():\n",
    "            blob.delete()\n",
    "            tqdm.write(f\"Deleted {file_name} from GCS.\")\n",
    "\n",
    "    def load_data_to_bigquery(self, source_uri):\n",
    "        dataset_ref = self.bq_client.dataset(self.dataset_name)\n",
    "        table_ref = dataset_ref.table(self.table_name)\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.PARQUET,\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_APPEND\n",
    "        )\n",
    "        \n",
    "        load_job = self.bq_client.load_table_from_uri(\n",
    "            source_uri, table_ref, job_config=job_config\n",
    "        )\n",
    "\n",
    "        # ロードジョブが完了するまで待つ\n",
    "        load_job.result()\n",
    "\n",
    "        tqdm.write(f\"Data loaded into BigQuery table {self.dataset_name}.{self.table_name} from {source_uri}.\")\n",
    "\n",
    "def suppress_yfinance_warnings():\n",
    "    import logging\n",
    "    yf_logger = logging.getLogger(\"yfinance\")\n",
    "    yf_logger.setLevel(logging.ERROR)\n",
    "\n",
    "def main():\n",
    "    suppress_yfinance_warnings()\n",
    "\n",
    "    # BigQueryとGCSの設定\n",
    "    PROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\")\n",
    "    DATASET_NAME = 'stock_dataset'\n",
    "    BUCKET_NAME = 'stock-data-bucket_hopop'\n",
    "    TABLE_NAME = 'stock_data'\n",
    "\n",
    "    # GCS & BigQuery操作のためのファサードを初期化\n",
    "    gcs_bq = GCSBigQueryFacade(PROJECT_ID, DATASET_NAME, TABLE_NAME, BUCKET_NAME)\n",
    "\n",
    "    # BigQueryから最大の日付を取得\n",
    "    max_date = gcs_bq.get_max_date_from_bq()\n",
    "    \n",
    "    if max_date:\n",
    "        # max_dateを文字列からdatetime型に変換\n",
    "        max_date = pd.to_datetime(max_date).date()\n",
    "        START_DATE = max_date + dt.timedelta(days=1)\n",
    "    else:\n",
    "        # データがない場合、デフォルトの開始日\n",
    "        START_DATE = dt.date(2024, 1, 1)\n",
    "\n",
    "    END_DATE = dt.date.today()\n",
    "\n",
    "    # START_DATE == END_DATEの場合、処理をスキップ\n",
    "    if START_DATE >= END_DATE:\n",
    "        tqdm.write(\"最新データが既に存在します。新しいデータはありません。\")\n",
    "        return\n",
    "\n",
    "    # GCS上のファイルが存在するかを確認し、削除\n",
    "    combined_file_name = \"combined_stock_data.parquet\"\n",
    "    gcs_bq.delete_from_gcs(combined_file_name)\n",
    "\n",
    "    # CSVファイルのパス\n",
    "    STOCK_MAPPING_CSV = '../stock_code_name_mapping.csv'\n",
    "\n",
    "    # CSVファイルの読み込み\n",
    "    stock_names_df = pd.read_csv(STOCK_MAPPING_CSV, usecols=['code', 'name'])\n",
    "\n",
    "    # すべての銘柄のデータを結合するためのデータフレームを準備\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # tqdmを使用して進捗を可視化（バーを最上部に固定）\n",
    "    progress_bar = tqdm(stock_names_df.iterrows(), total=len(stock_names_df), ncols=100, leave=True, position=0)\n",
    "\n",
    "    for index, row in progress_bar:\n",
    "        stock_code = str(row['code']).strip()\n",
    "        ticker = f\"{stock_code}.T\"  # 東証の場合、ティッカーは通常「.T」が付加されます\n",
    "        \n",
    "        # 株価データの取得\n",
    "        df = yf.download(ticker, start=START_DATE, end=END_DATE)\n",
    "\n",
    "        if df.empty:\n",
    "            tqdm.write(f\"No data found for {ticker}. Skipping...\")\n",
    "        else:\n",
    "            # データフレームの前処理\n",
    "            df.reset_index(inplace=True)\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Date',\n",
    "                'Open': 'Open',\n",
    "                'High': 'High',\n",
    "                'Low': 'Low',\n",
    "                'Close': 'Close',\n",
    "                'Adj Close': 'Adj_Close',\n",
    "                'Volume': 'Volume'\n",
    "            })\n",
    "            df['Stock_Code'] = stock_code\n",
    "    \n",
    "            # 必要なカラムのみを選択\n",
    "            df = df[['Date', 'Stock_Code', 'Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']]\n",
    "            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # データを結合\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        \n",
    "        # Yahoo Finance APIの制限を避けるために1秒スリープ\n",
    "        time.sleep(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "\n",
    "    if not combined_df.empty:\n",
    "        # すべての銘柄データを一度にまとめてParquetファイルとして保存\n",
    "        local_file_name = f\"combined_stock_data.parquet\"\n",
    "        local_file_path = f\"./output/{local_file_name}\"\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        \n",
    "        # Parquet形式で保存\n",
    "        combined_df.to_parquet(local_file_path, engine='pyarrow', index=False)\n",
    "        \n",
    "        # GCSへのアップロード\n",
    "        gcs_bq.upload_to_gcs(local_file_path, combined_file_name)\n",
    "        \n",
    "        # BigQueryへのデータロード\n",
    "        source_uri = f\"gs://{BUCKET_NAME}/{combined_file_name}\"\n",
    "        gcs_bq.load_data_to_bigquery(source_uri)\n",
    "    else:\n",
    "        tqdm.write(\"combined_dfが空です。処理をスキップします。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
