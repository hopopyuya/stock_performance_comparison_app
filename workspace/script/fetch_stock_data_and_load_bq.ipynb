{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9844b1e-33e0-4191-989f-70c46861c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas yfinance pyarrow google-cloud-bigquery google-cloud-storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from modules.gcp_class import Gcs_client, Bigquery_client\n",
    "import datetime as dt\n",
    "\n",
    "def main():\n",
    "    # 環境変数で認証情報を設定\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./dbt-analytics-engineer-435907-75a25995915e.json\"\n",
    "    \n",
    "    # BigQueryとGCSの設定\n",
    "    PROJECT_ID = 'dbt-analytics-engineer-435907'\n",
    "    DATASET_NAME = 'stock_dataset'\n",
    "    BUCKET_NAME = 'stock-data-bucket_hopop'\n",
    "    TABLE_NAME = 'stock_data'\n",
    "\n",
    "    # CSVファイルのパス\n",
    "    STOCK_MAPPING_CSV = '../stock_code_name_mapping.csv'\n",
    "\n",
    "    # データ取得期間の設定\n",
    "    START_DATE = dt.date(2024, 1, 1)\n",
    "    END_DATE = dt.date.today()\n",
    "\n",
    "    # GCSクライアントとBigQueryクライアントの初期化\n",
    "    gcs_client = Gcs_client()\n",
    "    bq_client = Bigquery_client()\n",
    "\n",
    "    # CSVファイルの読み込み\n",
    "    stock_names_df = pd.read_csv(STOCK_MAPPING_CSV, usecols=['code', 'name'])\n",
    "\n",
    "    # 既存のGCSオブジェクトのリストを取得（重複防止用）\n",
    "    existing_objects = set(gcs_client.list_all_objects(BUCKET_NAME))\n",
    "\n",
    "    # 各銘柄のデータを取得し、GCSにアップロード\n",
    "    for index, row in stock_names_df.loc[(stock_names_df.index >= 3970)].iterrows():\n",
    "        stock_code = str(row['code']).strip()\n",
    "        stock_name = row['name'].strip()\n",
    "        ticker = f\"{stock_code}.T\"  # 東証の場合、ティッカーは通常「.T」が付加されます\n",
    "        \n",
    "        # 株価データの取得\n",
    "        df = yf.download(ticker, start=START_DATE, end=END_DATE)\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No data found for {ticker}. Skipping...\")\n",
    "        else:\n",
    "            # データフレームの前処理\n",
    "            df.reset_index(inplace=True)\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Date',\n",
    "                'Open': 'Open',\n",
    "                'High': 'High',\n",
    "                'Low': 'Low',\n",
    "                'Close': 'Close',\n",
    "                'Adj Close': 'Adj_Close',\n",
    "                'Volume': 'Volume'\n",
    "            })\n",
    "            df['Stock_Code'] = stock_code\n",
    "    \n",
    "            # 必要なカラムのみを選択\n",
    "            df = df[['Date', 'Stock_Code', 'Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']]\n",
    "            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "            # Parquetファイルのパス設定\n",
    "            local_file_name = f\"{stock_code}.parquet\"\n",
    "            local_file_path = f\"./output/{local_file_name}\"\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "    \n",
    "            # Parquet形式で保存\n",
    "            df.to_parquet(local_file_path, engine='pyarrow', index=False)\n",
    "    \n",
    "            # GCSへのアップロード\n",
    "            if local_file_name in existing_objects:\n",
    "                print(f\"File {local_file_name} already exists in GCS. Skipping upload.\")\n",
    "            else:\n",
    "                gcs_client.upload_gcs(BUCKET_NAME, local_file_path, local_file_name)\n",
    "    \n",
    "            # BigQueryへのデータロード\n",
    "            table_id = f\"{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}\"\n",
    "            source_uri = f\"gs://{BUCKET_NAME}/{local_file_name}\"\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                source_format=bigquery.SourceFormat.PARQUET,\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_APPEND  # 既存データに追加\n",
    "            )\n",
    "            \n",
    "            load_job = bq_client.client.load_table_from_uri(\n",
    "                source_uri,\n",
    "                table_id,\n",
    "                job_config=job_config\n",
    "            )\n",
    "            \n",
    "            load_job.result()  # ジョブの完了を待つ\n",
    "            print(f\"Loaded data into BigQuery table {table_id} from {source_uri}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3138af4-a5d7-4693-9cfa-960654bc0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "STOCK_MAPPING_CSV = '../stock_code_name_mapping.csv'\n",
    "stock_names_df = pd.read_csv(STOCK_MAPPING_CSV, usecols=['code', 'name'])\n",
    "\n",
    "stock_names_df[stock_names_df['code']==8129]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
